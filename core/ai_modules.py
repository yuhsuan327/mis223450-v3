import os
import re
import json
import warnings
from dotenv import load_dotenv
from openai import OpenAI
from .models import Lecture, Question

load_dotenv()


def create_openai_client():
    api_key = os.getenv("OPENAI_API_KEY")
    api_base = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
    if not api_key or api_key.strip().upper() == "EMPTY":
        raise ValueError("âŒ è«‹è¨­å®š OPENAI_API_KEY")
    return OpenAI(api_key=api_key, base_url=api_base)


def transcribe_with_whisper(audio_path):
    try:
        if not os.path.exists(audio_path):
            print(f"âŒ æ‰¾ä¸åˆ°éŸ³è¨Šæª”æ¡ˆï¼š{audio_path}")
            return None
        print("âœ… Whisper API è½‰éŒ„é–‹å§‹")
        client = create_openai_client()
        with open(audio_path, "rb") as f:
            response = client.audio.transcriptions.create(
                model="whisper-1",
                file=f
            )
        return response.text
    except Exception as e:
        print(f"âŒ Whisper API è½‰éŒ„éŒ¯èª¤: {e}")
        return None


def dynamic_split(text, min_length=300, max_length=1000):
    text = text.strip()
    if len(text) <= max_length:
        return [text]
    paragraphs = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ])\s*', text)
    chunks, temp = [], ""
    for para in paragraphs:
        if len(temp) + len(para) <= max_length:
            temp += para
        else:
            if len(temp) >= min_length:
                chunks.append(temp.strip())
                temp = para
            else:
                temp += para
    if temp:
        chunks.append(temp.strip())
    return chunks


def generate_summary_for_chunk(client, chunk, chunk_index, total_chunks):
    prompt = [
        {"role": "system", "content": f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ç¹é«”ä¸­æ–‡èª²ç¨‹æ‘˜è¦è¨­è¨ˆå¸«ã€‚
è«‹é‡å°ç¬¬ {chunk_index + 1} æ®µèª²ç¨‹å…§å®¹é€²è¡Œé‡é»æ‘˜è¦ï¼ŒåŒ…å«ï¼š
- ç°¡æ½”å…§å®¹æ¦‚è¿°ï¼ˆ50â€“80å­—ï¼‰
- 2â€“4 å€‹å­¸ç¿’è¦é»ï¼Œä½¿ç”¨æ¢åˆ—å¼
ç¸½å­—æ•¸æ§åˆ¶åœ¨ 150 å­—å…§ã€‚"""},
        {"role": "user", "content": chunk}
    ]
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=prompt,
            temperature=0.3,
            max_tokens=400
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ æ®µè½æ‘˜è¦éŒ¯èª¤: {e}")
        return f"ç¬¬ {chunk_index + 1} æ®µæ‘˜è¦å¤±æ•—"


def combine_summaries(client, summaries):
    combined = "\n\n".join([f"æ®µè½ {i+1}ï¼š{s}" for i, s in enumerate(summaries)])
    prompt = [
        {"role": "system", "content": """ä½ æ˜¯æ•™è‚²è¨­è¨ˆå°ˆå®¶ï¼Œè«‹å°‡ä¸‹åˆ—åˆ†æ®µæ‘˜è¦çµ±æ•´ç‚ºå®Œæ•´èª²ç¨‹æ‘˜è¦ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

ã€èª²ç¨‹æ¦‚è¿°ã€‘ï¼šèªªæ˜æ•´é«”èª²ç¨‹å…§å®¹èˆ‡é‡è¦æ€§ï¼ˆ150â€“200å­—ï¼‰
ã€å­¸ç¿’é‡é»ã€‘ï¼šåˆ—å‡ºæœ¬èª²ç¨‹çš„ 4â€“5 å€‹å­¸ç¿’ç›®æ¨™ï¼ˆæ¢åˆ—ï¼‰
ã€å®Œæˆå¾Œæ”¶ç©«ã€‘ï¼šç°¡è¿°å­¸ç”Ÿå®Œæˆèª²ç¨‹å¾Œèƒ½å…·å‚™çš„èƒ½åŠ›ï¼ˆ60å­—å…§ï¼‰

è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œé¿å…é‡è¤‡æ•˜è¿°ï¼Œæ§åˆ¶ç¸½å­—æ•¸åœ¨ 400 å­—å…§ã€‚"""},
        {"role": "user", "content": combined}
    ]
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=prompt,
            temperature=0.3,
            max_tokens=512
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ ç¸½çµéŒ¯èª¤: {e}")
        return "æ•´åˆæ‘˜è¦å¤±æ•—"


# ğŸ†• æ”¹å–„ç‰ˆ generate_quiz åŠ å…¥è‡ªå‹•é‡è©¦èˆ‡è§£æè™•ç†
def safe_json_parse(raw: str):
    try:
        return normalize_mcq_payload(json.loads(raw))
    except Exception:
        pass

    m = re.search(r'```json\s*([\s\S]*?)```', raw, re.IGNORECASE)
    if not m:
        m = re.search(r'(\{[\s\S]*\}|\[[\s\S]*\])', raw)
    if m:
        candidate = m.group(1)
        candidate = candidate.replace("â€œ", '"').replace("â€", '"').replace("â€™", "'")
        candidate = re.sub(r",\s*([\]}])", r"\1", candidate)
        try:
            return normalize_mcq_payload(json.loads(candidate))
        except Exception as e:
            print("âŒ JSON å€å¡Šè§£æä»å¤±æ•—ï¼š", e)
    print("âš ï¸ MCQ åŸå§‹å›æ‡‰ï¼ˆæˆªæ–· 500 å­—ï¼‰ï¼š", raw[:500])
    raise ValueError("æ¨¡å‹æœªå›å‚³åˆæ³• JSON")


def normalize_mcq_payload(data):
    if isinstance(data, dict) and "items" in data and isinstance(data["items"], list):
        items = data["items"]
    elif isinstance(data, list):
        items = data
    else:
        raise ValueError("JSON çµæ§‹ä¸ç¬¦åˆé æœŸ")

    cleaned = []
    for it in items:
        try:
            concept = it["concept"].strip()
            question = it["question"].strip()
            options = it["options"]
            answer = it["answer"].strip().upper()
            explanation = it.get("explanation", "").strip()
            if not all(k in options for k in ["A", "B", "C", "D"]):
                continue
            if answer not in {"A", "B", "C", "D"}:
                continue
            cleaned.append({
                "concept": concept,
                "question": question,
                "options": {
                    "A": str(options["A"]),
                    "B": str(options["B"]),
                    "C": str(options["C"]),
                    "D": str(options["D"]),
                },
                "answer": answer,
                "explanation": explanation,
            })
        except Exception:
            continue
    return cleaned


def generate_quiz_with_retry(client, summary, count=3, retries=1):
    system = f"""ä½ æ˜¯ä¸€ä½èª²ç¨‹å‡ºé¡Œ AIï¼Œè«‹æ ¹æ“šä»¥ä¸‹èª²ç¨‹æ‘˜è¦ç”¢ç”Ÿ {count} é¡Œé¸æ“‡é¡Œã€‚
åš´æ ¼ä¸”åªè¼¸å‡º JSONã€Œé™£åˆ—ã€ï¼Œä¸è¦ä»»ä½•èªªæ˜ã€ä¸è¦åŠ  ```jsonã€‚æ¯ä¸€é¡Œç‰©ä»¶å¿…é ˆåŒ…å«ï¼š
concept, question, options(A/B/C/D), answer(åªèƒ½æ˜¯ A/B/C/D), explanationã€‚"""
    messages = [
        {"role": "system", "content": system},
        {"role": "user", "content": summary}
    ]
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            temperature=0.2,
            max_tokens=1500
        )
        raw = response.choices[0].message.content or ""
        return safe_json_parse(raw)
    except Exception as e:
        print("ğŸ” ç¬¬ä¸€æ¬¡è§£æå¤±æ•—ï¼Œæ”¹ç”¨æ›´åš´æ ¼æç¤ºé‡è©¦ï¼š", e)

    hard_prompt = summary + f"\n\nè«‹åš´æ ¼è¼¸å‡º {count} é¡Œé¸æ“‡é¡Œï¼Œåªè¼¸å‡º JSON é™£åˆ—ï¼Œæ ¼å¼åš´è¬¹ã€‚ä¸è¦æœ‰ä»»ä½•èªªæ˜ã€```jsonã€å‰å¾Œæ–‡å­—ã€‚"
    try:
        messages = [
            {"role": "system", "content": "ä½ åªæœƒè¼¸å‡º JSONã€‚"},
            {"role": "user", "content": hard_prompt}
        ]
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            temperature=0.1,
            max_tokens=1500
        )
        raw = response.choices[0].message.content or ""
        return safe_json_parse(raw)
    except Exception as e:
        print("âŒ é‡è©¦ä»å¤±æ•—ï¼š", e)
        return []


def generate_tf_questions(client, summary, count):
    prompt = [
        {
            "role": "system",
            "content": f"""è«‹æ ¹æ“šä»¥ä¸‹èª²ç¨‹æ‘˜è¦ï¼Œè¨­è¨ˆ {count} é¡Œæ˜¯éé¡Œï¼ˆTrue/Falseï¼‰ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
[
  {{
    "concept": "å­¸ç¿’æ¦‚å¿µ",
    "question": "å•é¡Œå…§å®¹",
    "answer": "True",
    "explanation": "æ­£ç¢ºç­”æ¡ˆè§£æ"
  }},
  ...
]
è«‹å›å‚³ JSON æ ¼å¼è³‡æ–™ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—èªªæ˜ã€‚"""
        },
        {"role": "user", "content": summary}
    ]
    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=prompt,
            temperature=0.5
        )
        return json.loads(response.choices[0].message.content)
    except Exception as e:
        print(f"âŒ æ˜¯éé¡Œç”¢ç”Ÿå¤±æ•—ï¼š{e}")
        return []


def parse_and_store_questions(summary, quiz_data, lecture, question_type):
    for item in quiz_data:
        question_text = item.get('question', '').strip()
        explanation = item.get('explanation', '').strip()
        answer = item.get('answer', '').strip()

        if question_type == 'mcq':
            options = item.get('options', {})
            Question.objects.create(
                lecture=lecture,
                question_text=question_text,
                option_a=options.get('A'),
                option_b=options.get('B'),
                option_c=options.get('C'),
                option_d=options.get('D'),
                correct_answer=answer,
                explanation=explanation,
                question_type='mcq'
            )
        elif question_type == 'tf':
            Question.objects.create(
                lecture=lecture,
                question_text=item.get('question', '').strip(),
                correct_answer=item.get('answer'),
                explanation=item.get('explanation', '').strip(),
                question_type='tf'
            )
        else:
            print(f"âš ï¸ æœªçŸ¥é¡Œå‹ï¼š{question_type}")


def process_audio_and_generate_quiz(lecture_id, num_mcq=3, num_tf=0):
    lecture = Lecture.objects.get(id=lecture_id)
    client = create_openai_client()

    print("ğŸ§ é–‹å§‹èªéŸ³è½‰éŒ„")
    transcript = transcribe_with_whisper(lecture.audio_file.path)
    if not transcript:
        return
    lecture.transcript = transcript
    lecture.save()

    print("ğŸ“ é–‹å§‹æ‘˜è¦è™•ç†")
    chunks = dynamic_split(transcript)
    summaries = [generate_summary_for_chunk(client, c, i, len(chunks)) for i, c in enumerate(chunks)]

    final_summary = combine_summaries(client, summaries)
    lecture.summary = final_summary
    lecture.save()

    print("ğŸ§  é–‹å§‹ç”¢ç”Ÿè€ƒé¡Œ")

    if num_mcq > 0:
        mcq_data = generate_quiz_with_retry(client, final_summary, num_mcq)
        if mcq_data:
            parse_and_store_questions(final_summary, mcq_data, lecture, 'mcq')
        else:
            print("âš ï¸ æ²’æœ‰å›å‚³ MCQ é¡Œç›®")

    if num_tf > 0:
        tf_data = generate_tf_questions(client, final_summary, num_tf)
        if tf_data:
            parse_and_store_questions(final_summary, tf_data, lecture, 'tf')
        else:
            print("âš ï¸ æ²’æœ‰å›å‚³ TF é¡Œç›®")


def process_transcript_and_generate_quiz(lecture, client=None, num_mcq=3, num_tf=0):
    if not client:
        client = create_openai_client()

    transcript = lecture.transcript
    if not transcript:
        print("âŒ ç„¡è½‰éŒ„å…§å®¹ï¼Œç„¡æ³•ç”Ÿæˆæ‘˜è¦èˆ‡é¡Œç›®")
        return

    print("ğŸ“ é–‹å§‹æ‘˜è¦è™•ç†")
    chunks = dynamic_split(transcript)
    summaries = [generate_summary_for_chunk(client, c, i, len(chunks)) for i, c in enumerate(chunks)]

    final_summary = combine_summaries(client, summaries)
    lecture.summary = final_summary
    lecture.save()

    print("ğŸ§  é–‹å§‹ç”¢ç”Ÿè€ƒé¡Œ")

    if num_mcq > 0:
        mcq_data = generate_quiz_with_retry(client, final_summary, num_mcq)
        if mcq_data:
            parse_and_store_questions(final_summary, mcq_data, lecture, 'mcq')
        else:
            print("âš ï¸ æ²’æœ‰å›å‚³ MCQ é¡Œç›®")

    if num_tf > 0:
        tf_data = generate_tf_questions(client, final_summary, num_tf)
        if tf_data:
            parse_and_store_questions(final_summary, tf_data, lecture, 'tf')
        else:
            print("âš ï¸ æ²’æœ‰å›å‚³ TF é¡Œç›®")

def process_transcript_and_generate_quiz(lecture, client=None, num_mcq=3, num_tf=0):
    if not client:
        client = create_openai_client()

    transcript = lecture.transcript
    if not transcript:
        print("âŒ ç„¡è½‰éŒ„å…§å®¹ï¼Œç„¡æ³•ç”Ÿæˆæ‘˜è¦èˆ‡é¡Œç›®")
        return

    print("ğŸ“ é–‹å§‹æ‘˜è¦è™•ç†")
    chunks = dynamic_split(transcript)
    summaries = [generate_summary_for_chunk(client, c, i, len(chunks)) for i, c in enumerate(chunks)]

    final_summary = combine_summaries(client, summaries)
    lecture.summary = final_summary
    lecture.save()

    print("ğŸ§  é–‹å§‹ç”¢ç”Ÿè€ƒé¡Œ")

    if num_mcq > 0:
        mcq_data = generate_quiz(client, final_summary, num_mcq)
        if mcq_data:
            parse_and_store_questions(final_summary, mcq_data, lecture, 'mcq')
        else:
            print("âš ï¸ æ²’æœ‰å›å‚³ MCQ é¡Œç›®")

    if num_tf > 0:
        tf_data = generate_tf_questions(client, final_summary, num_tf)
        if tf_data:
            parse_and_store_questions(final_summary, tf_data, lecture, 'tf')
        else:
            print("âš ï¸ æ²’æœ‰å›å‚³ TF é¡Œç›®")